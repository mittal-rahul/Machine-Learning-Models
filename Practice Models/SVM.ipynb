{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using SVM classifier on the Iris dataset. We will scale the features and then train a lienar SVM.\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X=iris[\"data\"][:, (2,3)] # petals length, petal width\n",
    "y = (iris[\"target\"]==2).astype(np.float64) # Iris virginica\n",
    "\n",
    "svm_clf = Pipeline([(\"scaler\", StandardScaler()), (\"linear_svc\", LinearSVC(C=1, loss =\"hinge\")),])\n",
    "svm_clf.fit(X,y)\n",
    "svm_clf.predict([[5.5,1.7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM output the class name, they do not output the probability like the logistic regression.\n",
    "#Instead of using the LinearSVC, we can use the SVC class with linear kernel. Ex: SVC(kernel=\"linear, C=1\").\n",
    "#We can also use the SGDClassifier class as SGDClassifier(loss=\"hinge\", alpha=1/(m*C)). This applies stochastic\n",
    "#gradient descent to train linear SVM. It does not converge as fast as LinearSVC class, but it can be useful to handle\n",
    "#online classification tasks or huge databases that do not fit in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('poly_features', PolynomialFeatures(degree=3, include_bias=True, interaction_only=False)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svm_clf', LinearSVC(C=10, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
       "     penalty='l2', random_state=None, tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Nonlinear SVM classification. It is similar to the polynomial logistic regression. we add the polynomial of features\n",
    "#and then apply the SVC. We will use the moons dataset which contains dataset of toys in two interleaving half circles.\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X,y = make_moons(n_samples=100, noise=0.15)\n",
    "polynomial_svm_clf=Pipeline([(\"poly_features\", PolynomialFeatures(degree=3)), (\"scaler\", StandardScaler()),\n",
    "                            (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\")),])\n",
    "polynomial_svm_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svm_clf', SVC(C=5, cache_size=200, class_weight=None, coef0=1,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='poly', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Polynomial Kernel. In this, we use the polynomial kernel of the SVC class that acts as if many high degree polynomial\n",
    "#features are added without actually adding them. It makes the model fast.\n",
    "from sklearn.svm import SVC\n",
    "poly_kernel_svm_clf = Pipeline([(\"scaler\", StandardScaler()), \n",
    "                                (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5)),])\n",
    "poly_kernel_svm_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The above code trains a SVM classifier using 3rd degree polynomial kernel. The hyperparameter coef0 controls how\n",
    "#much the model is influenced by high degree polynomials versus low degree polynomials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svm_clf', SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=5, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Gaussian RBF Kernel. It adds the similarity features in the training dataset which makes the non linear dataset\n",
    "#as linearly separable dataset.\n",
    "rbf_kernel_svm_clf = Pipeline([(\"scaler\", StandardScaler()), (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001)),])\n",
    "rbf_kernel_svm_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the above ,odel, the gamma is the regularization parameter. If the model is overfitting then we need to reduce the\n",
    "#gamma, If the model is underfitting then we need to increase the gamma.\n",
    "#There are other kernels for SVC but are rarely used are or specific to certain tasks. String kernels are used for\n",
    "#classifying text documents or DNA sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=1.0, dual=True, epsilon=1.5, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,\n",
       "     random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SVM algorithm also supports linear and non linear regression. It is controlled by hyperparameter epsilon.\n",
    "from sklearn.svm import LinearSVR\n",
    "svm_reg = LinearSVR(epsilon =1.5)\n",
    "svm_reg.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVR(C=100, cache_size=200, coef0=0.0, degree=2, epsilon=0.1,\n",
       "  gamma='auto_deprecated', kernel='poly', max_iter=-1, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To tackle nonlinear regression tasks, we use the kernelized SVM model.\n",
    "from sklearn.svm import SVR\n",
    "svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\n",
    "svm_poly_reg.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
